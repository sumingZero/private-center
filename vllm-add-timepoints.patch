From 45b7504276f643f2da970e962ae8c0d6a8223918 Mon Sep 17 00:00:00 2001
From: sumingZero <469434916@qq.com>
Date: Mon, 1 Dec 2025 18:19:38 -0800
Subject: [PATCH] add runtime points

---
 vllm/benchmarks/endpoint_request_func.py      | 23 ++++++++++++++++
 vllm/benchmarks/serve.py                      | 14 ++++++++++
 vllm/entrypoints/openai/protocol.py           |  2 ++
 vllm/entrypoints/openai/serving_completion.py |  3 +++
 vllm/v1/core/sched/scheduler.py               | 27 +++++++++++++++++++
 vllm/v1/outputs.py                            |  3 +++
 vllm/v1/worker/gpu_model_runner.py            | 15 +++++++++++
 vllm/v1/worker/gpu_worker.py                  |  4 +++
 8 files changed, 91 insertions(+)

diff --git a/vllm/benchmarks/endpoint_request_func.py b/vllm/benchmarks/endpoint_request_func.py
index 60ae520db..6a5065fc6 100644
--- a/vllm/benchmarks/endpoint_request_func.py
+++ b/vllm/benchmarks/endpoint_request_func.py
@@ -46,6 +46,15 @@ class RequestFuncOutput:
     tpot: float = 0.0  # avg next-token latencies
     prompt_len: int = 0
     error: str = ""
+    created_time: float = 0.0
+    send_time: float = 0.0
+    first_token_time: float = 0.0
+    running_time: float = 0.0
+    worker_time: float = 0.0
+    start_loadkv_time: float = 0.0
+    start_forward_time: float = 0.0
+    finish_forward_time: float = 0.0
+    finish_savekv_time: float = 0.0
 
 
 async def async_request_openai_completions(
@@ -95,6 +104,7 @@ async def async_request_openai_completions(
         generated_text = ""
         st = time.perf_counter()
         most_recent_timestamp = st
+        created_time = -1
         try:
             async with session.post(url=api_url, json=payload,
                                     headers=headers) as response:
@@ -115,6 +125,8 @@ async def async_request_openai_completions(
 
                         if chunk != "[DONE]":
                             data = json.loads(chunk)
+                            if created_time == -1:
+                                created_time = data.get("created")
 
                             # NOTE: Some completion API might have a last
                             # usage summary response without a token so we
@@ -127,6 +139,7 @@ async def async_request_openai_completions(
                                 # First token
                                 if not first_chunk_received:
                                     first_chunk_received = True
+                                    first_token_coming_time = time.perf_counter() * 1000
                                     ttft = time.perf_counter() - st
                                     output.ttft = ttft
 
@@ -140,6 +153,16 @@ async def async_request_openai_completions(
                             elif usage := data.get("usage"):
                                 output.output_tokens = usage.get(
                                     "completion_tokens")
+                                kv_transfer_params = data.get("kv_transfer_params")
+                                output.created_time = created_time
+                                output.send_time = st * 1000
+                                output.first_token_time = first_token_coming_time
+                                output.running_time = kv_transfer_params.get("running_time")
+                                output.worker_time = kv_transfer_params.get("worker_time")
+                                output.start_loadkv_time = kv_transfer_params.get("start_loadkv_time")
+                                output.start_forward_time = kv_transfer_params.get("start_forward_time")
+                                output.finish_forward_time = kv_transfer_params.get("finish_forward_time")
+                                output.finish_savekv_time = kv_transfer_params.get("finish_savekv_time")
                     if first_chunk_received:
                         output.success = True
                     else:
diff --git a/vllm/benchmarks/serve.py b/vllm/benchmarks/serve.py
index 8b16fea9e..cf30c93fd 100644
--- a/vllm/benchmarks/serve.py
+++ b/vllm/benchmarks/serve.py
@@ -508,6 +508,13 @@ async def benchmark(
         "itls": [output.itl for output in outputs],
         "generated_texts": [output.generated_text for output in outputs],
         "errors": [output.error for output in outputs],
+        "send_time": [output.send_time for output in outputs],
+        "running_time": [output.running_time for output in outputs],
+        "worker_time": [output.worker_time for output in outputs],
+        "start_loadkv_time": [output.start_loadkv_time for output in outputs],
+        "start_forward_time": [output.start_forward_time for output in outputs],
+        "finish_forward_time": [output.finish_forward_time for output in outputs],
+        "finish_savekv_time": [output.finish_savekv_time for output in outputs],
     }
 
     if rps_change_events:
@@ -1033,6 +1040,13 @@ def main(args: argparse.Namespace):
                     "itls",
                     "generated_texts",
                     "errors",
+                    "send_time",
+                    "running_time",
+                    "worker_time",
+                    "start_loadkv_time",
+                    "start_forward_time",
+                    "finish_forward_time",
+                    "finish_savekv_time",
             ]:
                 if field in result_json:
                     del result_json[field]
diff --git a/vllm/entrypoints/openai/protocol.py b/vllm/entrypoints/openai/protocol.py
index d4db238f4..7754c4538 100644
--- a/vllm/entrypoints/openai/protocol.py
+++ b/vllm/entrypoints/openai/protocol.py
@@ -1255,6 +1255,8 @@ class CompletionResponseStreamChoice(OpenAIBaseModel):
             "to stop, None if the completion finished for some other reason "
             "including encountering the EOS token"),
     )
+    kv_transfer_params: Optional[dict[str, Any]] = Field(
+        default=None, description="KVTransfer parameters.")
 
 
 class CompletionStreamResponse(OpenAIBaseModel):
diff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py
index 6c9c29b71..00628c5a1 100644
--- a/vllm/entrypoints/openai/serving_completion.py
+++ b/vllm/entrypoints/openai/serving_completion.py
@@ -325,6 +325,8 @@ class OpenAIServingCompletion(OpenAIServing):
 
         try:
             async for prompt_idx, res in result_generator:
+                if res.kv_transfer_params is not None:
+                    kv_transfer_params = res.kv_transfer_params
                 prompt_token_ids = res.prompt_token_ids
                 prompt_logprobs = res.prompt_logprobs
 
@@ -438,6 +440,7 @@ class OpenAIServingCompletion(OpenAIServing):
                     model=model_name,
                     choices=[],
                     usage=final_usage_info,
+                    kv_transfer_params = kv_transfer_params,
                 )
                 final_usage_data = (final_usage_chunk.model_dump_json(
                     exclude_unset=False, exclude_none=True))
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index fe552db74..dac28f2f3 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -495,6 +495,7 @@ class Scheduler(SchedulerInterface):
                 num_scheduled_tokens[request.request_id] = num_new_tokens
                 token_budget -= num_new_tokens
                 request.status = RequestStatus.RUNNING
+                request.running_time = time.perf_counter() * 1000
                 request.num_computed_tokens = num_computed_tokens
                 # Count the number of prefix cached tokens.
                 if request.num_cached_tokens < 0:
@@ -792,6 +793,14 @@ class Scheduler(SchedulerInterface):
             new_token_ids = generated_token_ids
             kv_transfer_params = None
 
+            is_prefill = request.num_output_tokens == 0
+            if is_prefill:
+                request.worker_time = model_runner_output.time_points["worker_time"]
+                request.start_loadkv_time = model_runner_output.time_points["start_loadkv_time"]
+                request.start_forward_time = model_runner_output.time_points["start_forward_time"]
+                request.finish_forward_time = model_runner_output.time_points["finish_forward_time"]
+                request.finish_savekv_time = model_runner_output.time_points["finish_savekv_time"]
+
             # Append generated tokens and check for stop. Note that if
             # a request is still being prefilled, we expect the model runner
             # to return empty token ids for the request.
@@ -987,6 +996,24 @@ class Scheduler(SchedulerInterface):
         if not delay_free_blocks:
             self._free_blocks(request)
 
+        # Not for nixl
+        if kv_xfer_params is None:
+            kv_xfer_params = dict(
+                running_time=request.running_time,
+                worker_time=request.worker_time,
+                start_loadkv_time=request.start_loadkv_time,
+                start_forward_time=request.start_forward_time,
+                finish_forward_time=request.finish_forward_time,
+                finish_savekv_time=request.finish_savekv_time,
+            )
+        else:
+            kv_xfer_params["running_time"] = request.running_time
+            kv_xfer_params["worker_time"] = request.worker_time
+            kv_xfer_params["start_loadkv_time"] = request.start_loadkv_time
+            kv_xfer_params["start_forward_time"] = request.start_forward_time
+            kv_xfer_params["finish_forward_time"] = request.finish_forward_time
+            kv_xfer_params["finish_savekv_time"] = request.finish_savekv_time
+
         return kv_xfer_params
 
     def _free_blocks(self, request: Request):
diff --git a/vllm/v1/outputs.py b/vllm/v1/outputs.py
index f78623f57..36613b556 100644
--- a/vllm/v1/outputs.py
+++ b/vllm/v1/outputs.py
@@ -111,6 +111,9 @@ class ModelRunnerOutput:
     # req_id -> num_nans_in_logits
     num_nans_in_logits: Optional[dict[str, int]] = None
 
+    # runtime time points
+    time_points: Optional[dict[str, float]] = None
+
 
 EMPTY_MODEL_RUNNER_OUTPUT = ModelRunnerOutput(req_ids=[],
                                               req_id_to_index={},
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 5a26e88db..becc9e755 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -1369,7 +1369,9 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 num_tokens_across_dp=num_tokens_across_dp,
                 skip_cuda_graphs=skip_cuda_graphs,
         ):
+            start_loadkv_time = time.perf_counter() * 1000
             self.maybe_setup_kv_connector(scheduler_output)
+            start_forward_time = time.perf_counter() * 1000
 
             model_output = self.model(
                 input_ids=input_ids,
@@ -1378,7 +1380,19 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 inputs_embeds=inputs_embeds,
             )
 
+            torch.cuda.synchronize()
+            finish_forward_time = time.perf_counter() * 1000
+
             self.maybe_wait_for_kv_save()
+
+            finish_savekv_time = time.perf_counter() * 1000
+            time_points = {
+                "start_loadkv_time": start_loadkv_time,
+                "start_forward_time": start_forward_time,
+                "finish_forward_time": finish_forward_time,
+                "finish_savekv_time": finish_savekv_time,
+            }
+
             finished_sending, finished_recving = (
                 self.get_finished_kv_transfers(scheduler_output))
 
@@ -1563,6 +1577,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             finished_sending=finished_sending,
             finished_recving=finished_recving,
             num_nans_in_logits=num_nans_in_logits,
+            time_points = time_points
         )
 
     def propose_draft_token_ids(
diff --git a/vllm/v1/worker/gpu_worker.py b/vllm/v1/worker/gpu_worker.py
index 9e7e44d06..086c559ff 100644
--- a/vllm/v1/worker/gpu_worker.py
+++ b/vllm/v1/worker/gpu_worker.py
@@ -3,6 +3,7 @@
 """A GPU worker class."""
 import gc
 import os
+import time
 from typing import TYPE_CHECKING, Optional
 
 import torch
@@ -305,8 +306,11 @@ class Worker(WorkerBase):
                 get_pp_group().recv_tensor_dict(
                     all_gather_group=get_tp_group()))
 
+        worker_time = time.perf_counter() * 1000
         output = self.model_runner.execute_model(scheduler_output,
                                                  intermediate_tensors)
+        if output.time_points:
+            output.time_points["worker_time"] = worker_time
         parallel_config = self.vllm_config.parallel_config
         if parallel_config.distributed_executor_backend != "external_launcher" \
             and not get_pp_group().is_last_rank:
-- 
2.34.1

