From 9ac20d82319936ea022b19f9f51da9ae72fb0e5c Mon Sep 17 00:00:00 2001
From: sumingZero <469434916@qq.com>
Date: Tue, 11 Nov 2025 19:12:50 -0800
Subject: [PATCH] add runtime time points

---
 vllm/benchmarks/endpoint_request_func.py      | 14 +++++++++
 vllm/benchmarks/serve.py                      | 14 +++++++++
 vllm/entrypoints/openai/protocol.py           |  2 ++
 vllm/entrypoints/openai/serving_completion.py |  3 ++
 vllm/v1/core/sched/scheduler.py               | 31 +++++++++++++++++++
 vllm/v1/outputs.py                            |  3 ++
 vllm/v1/worker/gpu_model_runner.py            | 16 ++++++++--
 vllm/v1/worker/gpu_worker.py                  |  6 +++-
 8 files changed, 86 insertions(+), 3 deletions(-)

diff --git a/vllm/benchmarks/endpoint_request_func.py b/vllm/benchmarks/endpoint_request_func.py
index 60ae520db..05b5261c2 100644
--- a/vllm/benchmarks/endpoint_request_func.py
+++ b/vllm/benchmarks/endpoint_request_func.py
@@ -95,6 +95,7 @@ async def async_request_openai_completions(
         generated_text = ""
         st = time.perf_counter()
         most_recent_timestamp = st
+        created_time = -1
         try:
             async with session.post(url=api_url, json=payload,
                                     headers=headers) as response:
@@ -115,6 +116,8 @@ async def async_request_openai_completions(
 
                         if chunk != "[DONE]":
                             data = json.loads(chunk)
+                            if created_time == -1:
+                                created_time = data.get("created")
 
                             # NOTE: Some completion API might have a last
                             # usage summary response without a token so we
@@ -127,6 +130,7 @@ async def async_request_openai_completions(
                                 # First token
                                 if not first_chunk_received:
                                     first_chunk_received = True
+                                    first_token_coming_time = time.perf_counter() * 1000
                                     ttft = time.perf_counter() - st
                                     output.ttft = ttft
 
@@ -140,6 +144,16 @@ async def async_request_openai_completions(
                             elif usage := data.get("usage"):
                                 output.output_tokens = usage.get(
                                     "completion_tokens")
+                                kv_transfer_params = data.get("kv_transfer_params")
+                                output.created_time = created_time
+                                output.send_time = st * 1000
+                                output.first_token_time = first_token_coming_time
+                                output.running_time = kv_transfer_params.get("running_time")
+                                output.worker_time = kv_transfer_params.get("worker_time")
+                                output.start_loadkv_time = kv_transfer_params.get("start_loadkv_time")
+                                output.start_forward_time = kv_transfer_params.get("start_forward_time")
+                                output.finish_forward_time = kv_transfer_params.get("finish_forward_time")
+                                output.finish_savekv_time = kv_transfer_params.get("finish_savekv_time")
                     if first_chunk_received:
                         output.success = True
                     else:
diff --git a/vllm/benchmarks/serve.py b/vllm/benchmarks/serve.py
index 8b16fea9e..cf30c93fd 100644
--- a/vllm/benchmarks/serve.py
+++ b/vllm/benchmarks/serve.py
@@ -508,6 +508,13 @@ async def benchmark(
         "itls": [output.itl for output in outputs],
         "generated_texts": [output.generated_text for output in outputs],
         "errors": [output.error for output in outputs],
+        "send_time": [output.send_time for output in outputs],
+        "running_time": [output.running_time for output in outputs],
+        "worker_time": [output.worker_time for output in outputs],
+        "start_loadkv_time": [output.start_loadkv_time for output in outputs],
+        "start_forward_time": [output.start_forward_time for output in outputs],
+        "finish_forward_time": [output.finish_forward_time for output in outputs],
+        "finish_savekv_time": [output.finish_savekv_time for output in outputs],
     }
 
     if rps_change_events:
@@ -1033,6 +1040,13 @@ def main(args: argparse.Namespace):
                     "itls",
                     "generated_texts",
                     "errors",
+                    "send_time",
+                    "running_time",
+                    "worker_time",
+                    "start_loadkv_time",
+                    "start_forward_time",
+                    "finish_forward_time",
+                    "finish_savekv_time",
             ]:
                 if field in result_json:
                     del result_json[field]
diff --git a/vllm/entrypoints/openai/protocol.py b/vllm/entrypoints/openai/protocol.py
index d4db238f4..7754c4538 100644
--- a/vllm/entrypoints/openai/protocol.py
+++ b/vllm/entrypoints/openai/protocol.py
@@ -1255,6 +1255,8 @@ class CompletionResponseStreamChoice(OpenAIBaseModel):
             "to stop, None if the completion finished for some other reason "
             "including encountering the EOS token"),
     )
+    kv_transfer_params: Optional[dict[str, Any]] = Field(
+        default=None, description="KVTransfer parameters.")
 
 
 class CompletionStreamResponse(OpenAIBaseModel):
diff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py
index 6c9c29b71..00628c5a1 100644
--- a/vllm/entrypoints/openai/serving_completion.py
+++ b/vllm/entrypoints/openai/serving_completion.py
@@ -325,6 +325,8 @@ class OpenAIServingCompletion(OpenAIServing):
 
         try:
             async for prompt_idx, res in result_generator:
+                if res.kv_transfer_params is not None:
+                    kv_transfer_params = res.kv_transfer_params
                 prompt_token_ids = res.prompt_token_ids
                 prompt_logprobs = res.prompt_logprobs
 
@@ -438,6 +440,7 @@ class OpenAIServingCompletion(OpenAIServing):
                     model=model_name,
                     choices=[],
                     usage=final_usage_info,
+                    kv_transfer_params = kv_transfer_params,
                 )
                 final_usage_data = (final_usage_chunk.model_dump_json(
                     exclude_unset=False, exclude_none=True))
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index 2d4fd4d59..b3f35aa6b 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -497,6 +497,7 @@ class Scheduler(SchedulerInterface):
                 num_scheduled_tokens[request.request_id] = num_new_tokens
                 token_budget -= num_new_tokens
                 request.status = RequestStatus.RUNNING
+                request.running_time = time.perf_counter() * 1000
                 request.num_computed_tokens = num_computed_tokens
                 # Count the number of prefix cached tokens.
                 if request.num_cached_tokens < 0:
@@ -809,6 +810,19 @@ class Scheduler(SchedulerInterface):
             new_logprobs = None
             new_token_ids = generated_token_ids
             kv_transfer_params = None
+
+            is_prefill = request.num_output_tokens == 0
+            if model_runner_output.finished_dumping is not None:
+                request.succeed_dumped_blocks.extend(model_runner_output.finished_dumping.get(req_id, []))
+                if is_prefill:
+                    self.connector.connector.commit(model_runner_output.finished_dumping.get(req_id, []), True)
+            if is_prefill:
+                request.worker_time = model_runner_output.time_points["worker_time"]
+                request.start_loadkv_time = model_runner_output.time_points["start_loadkv_time"]
+                request.start_forward_time = model_runner_output.time_points["start_forward_time"]
+                request.finish_forward_time = model_runner_output.time_points["finish_forward_time"]
+                request.finish_savekv_time = model_runner_output.time_points["finish_savekv_time"]
+
             if model_runner_output.finished_dumping is not None:
                 request.succeed_dumped_blocks.extend(model_runner_output.finished_dumping.get(req_id, []))
                 is_prefill = request.num_output_tokens == 0
@@ -1014,6 +1028,23 @@ class Scheduler(SchedulerInterface):
 
         if not delay_free_blocks:
             self._free_blocks(request)
+        # Not for nixl
+        if kv_xfer_params is None:
+            kv_xfer_params = dict(
+                running_time=request.running_time,
+                worker_time=request.worker_time,
+                start_loadkv_time=request.start_loadkv_time,
+                start_forward_time=request.start_forward_time,
+                finish_forward_time=request.finish_forward_time,
+                finish_savekv_time=request.finish_savekv_time,
+            )
+        else:
+            kv_xfer_params["running_time"] = request.running_time
+            kv_xfer_params["worker_time"] = request.worker_time
+            kv_xfer_params["start_loadkv_time"] = request.start_loadkv_time
+            kv_xfer_params["start_forward_time"] = request.start_forward_time
+            kv_xfer_params["finish_forward_time"] = request.finish_forward_time
+            kv_xfer_params["finish_savekv_time"] = request.finish_savekv_time
 
         return kv_xfer_params
 
diff --git a/vllm/v1/outputs.py b/vllm/v1/outputs.py
index 16af8dbce..ccb2de827 100644
--- a/vllm/v1/outputs.py
+++ b/vllm/v1/outputs.py
@@ -116,6 +116,9 @@ class ModelRunnerOutput:
     # req_id -> num_nans_in_logits
     num_nans_in_logits: Optional[dict[str, int]] = None
 
+    # runtime time points
+    time_points: Optional[dict[str, float]] = None
+
 
 EMPTY_MODEL_RUNNER_OUTPUT = ModelRunnerOutput(req_ids=[],
                                               req_id_to_index={},
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index c3df1d5d2..95050ba87 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -1385,7 +1385,9 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 num_tokens_across_dp=num_tokens_across_dp,
                 skip_cuda_graphs=skip_cuda_graphs,
         ):
+            start_loadkv_time = time.perf_counter() * 1000
             self.maybe_setup_kv_connector(scheduler_output)
+            start_forward_time = time.perf_counter() * 1000
 
             model_output = self.model(
                 input_ids=input_ids,
@@ -1394,7 +1396,17 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 inputs_embeds=inputs_embeds,
             )
 
+            torch.cuda.synchronize()
+            finish_forward_time = time.perf_counter() * 1000
             finished_dumping = self.maybe_wait_for_kv_save()
+            finish_savekv_time = time.perf_counter() * 1000
+            time_points = {
+                "start_loadkv_time": start_loadkv_time,
+                "start_forward_time": start_forward_time,
+                "finish_forward_time": finish_forward_time,
+                "finish_savekv_time": finish_savekv_time,
+            }
+
             finished_sending, finished_recving = (
                 self.get_finished_kv_transfers(scheduler_output))
             invalid_block_ids = self.get_block_ids_with_load_errors()
@@ -1568,7 +1580,6 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             get_kv_transfer_group().clear_connector_metadata()
 
         self.eplb_step()
-
         return ModelRunnerOutput(
             req_ids=self.input_batch.req_ids,
             req_id_to_index=self.input_batch.req_id_to_index,
@@ -1581,7 +1592,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             finished_recving=finished_recving,
             finished_dumping=finished_dumping,
             num_nans_in_logits=num_nans_in_logits,
-            invalid_block_ids = invalid_block_ids
+            invalid_block_ids = invalid_block_ids,
+            time_points = time_points
         )
 
     def propose_draft_token_ids(
diff --git a/vllm/v1/worker/gpu_worker.py b/vllm/v1/worker/gpu_worker.py
index 1b816b25b..3ee9ee3e4 100644
--- a/vllm/v1/worker/gpu_worker.py
+++ b/vllm/v1/worker/gpu_worker.py
@@ -4,6 +4,7 @@
 import copy
 import gc
 import os
+import time
 from typing import TYPE_CHECKING, Optional
 
 import torch
@@ -306,9 +307,12 @@ class Worker(WorkerBase):
             intermediate_tensors = IntermediateTensors(
                 get_pp_group().recv_tensor_dict(
                     all_gather_group=get_tp_group()))
-
+        
+        worker_time = time.perf_counter() * 1000
         output = self.model_runner.execute_model(scheduler_output,
                                                  intermediate_tensors)
+        if output.time_points:
+            output.time_points["worker_time"] = worker_time
         parallel_config = self.vllm_config.parallel_config
         if parallel_config.distributed_executor_backend != "external_launcher" \
             and not get_pp_group().is_last_rank:
-- 
2.34.1

